
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>End-to-End Example &#8212; MXMC 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Source Code Documentation" href="source_code.html" />
    <link rel="prev" title="Introduction" href="intro.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="end-to-end-example">
<h1>End-to-End Example<a class="headerlink" href="#end-to-end-example" title="Permalink to this headline">¶</a></h1>
<p>This example provides a simple demonstration of <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code> functionality. Here, the high-fidelity model considered is the Ishigami function, which is frequently used to test methods for uncertainty quantification:</p>
<div class="math notranslate nohighlight">
\[f^{(1)}\ =\ sin(z_1)\ +\ 5\ sin^{2}(z_2)\ +\ \frac{1}{10}\ z_3^{4} sin(z_1)\]</div>
<div class="math notranslate nohighlight">
\[with\ \ z_i \sim \textit{U} (-\pi,\ \pi)\]</div>
<p>The following two correlated functions [1] are treated as low-fidelity models to facilitate the use of multi-model estimators:</p>
<div class="math notranslate nohighlight">
\[f^{(2)}\ =\ sin(z_1)\ +\ 4.75\ sin^{2}(z_2)\ +\ \frac{1}{10}\ z_3^{4} sin(z_1)\]</div>
<div class="math notranslate nohighlight">
\[f^{(3)}\ =\ sin(z_1)\ +\ 3\ sin^{2}(z_2)\ +\ \frac{9}{10}\ z_3^{2} sin(z_1)\]</div>
<p>Furthermore, the costs associated with evaluating the functions <span class="math notranslate nohighlight">\(f^{(1)}\)</span>,  <span class="math notranslate nohighlight">\(f^{(2)}\)</span>, and  <span class="math notranslate nohighlight">\(f^{(3)}\)</span> are assumed to be 1.0, 0.05, 0.001 seconds, respectively. The goal is to estimate <span class="math notranslate nohighlight">\(E[f^{(1)}]\)</span> using <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code> by leveraging all three models and comparing with the analytical solution, <span class="math notranslate nohighlight">\(E[f^{(1)}] = 2.5\)</span>.</p>
<p>This example covers each step for utilizing <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code>: estimating the covariance of
model outputs from pilot samples, performing the sample allocation optimization,
and forming an estimator from outputs generated from each model according to
the optimal sample allocation.</p>
<p>The full source code for this example can be found in the <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code> repository:</p>
<p><code class="docutils literal notranslate"><span class="pre">/examples/ishigami/run_ishigami.py</span></code></p>
<div class="section" id="step-1-compute-model-outputs-for-pilot-samples">
<h2>Step 1: Compute model outputs for pilot samples<a class="headerlink" href="#step-1-compute-model-outputs-for-pilot-samples" title="Permalink to this headline">¶</a></h2>
<p>In order to determine the optimal sample allocation across the available models, <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code> needs the cost of each model as well as the covariance matrix of the outputs from each model. If the covariance matrix is unknown, it can be estimated from pilot samples, demonstrated here.</p>
<p>Starting from the beginning, the necessary Python modules are imported, including <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code> classes and Numpy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mxmc</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">mxmc</span> <span class="kn">import</span> <span class="n">OutputProcessor</span>
<span class="kn">from</span> <span class="nn">mxmc</span> <span class="kn">import</span> <span class="n">Estimator</span>
</pre></div>
</div>
<p>It is assumed that a user-defined class (<code class="docutils literal notranslate"><span class="pre">IshigamiModel</span></code>) implementing the Ishigami models and function (<code class="docutils literal notranslate"><span class="pre">get_uniform_sample_distribution</span></code>) for sampling the uniform random inputs are available. Three IshigamiModels are
then instantiated per the parameters in the equations above and
stored in a list variable named “models”:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ishigami_model</span> <span class="kn">import</span> <span class="n">IshigamiModel</span>

<span class="n">num_pilot_samples</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model_costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="o">.</span><span class="mi">001</span><span class="p">])</span>

<span class="n">high_fidelity_model</span> <span class="o">=</span>   <span class="n">IshigamiModel</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>   <span class="n">b</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">4.</span><span class="p">)</span>
<span class="n">medium_fidelity_model</span> <span class="o">=</span> <span class="n">IshigamiModel</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mf">4.75</span><span class="p">,</span> <span class="n">b</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">4.</span><span class="p">)</span>
<span class="n">low_fidelity_model</span> <span class="o">=</span>    <span class="n">IshigamiModel</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span>   <span class="n">b</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">high_fidelity_model</span><span class="p">,</span> <span class="n">medium_fidelity_model</span><span class="p">,</span> <span class="n">low_fidelity_model</span><span class="p">]</span>
</pre></div>
</div>
<p>Ten pilot samples are computed with each model as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pilot_inputs</span> <span class="o">=</span> <span class="n">get_uniform_sample_distribution</span><span class="p">(</span><span class="n">num_pilot_samples</span><span class="p">)</span>
<span class="n">pilot_outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="n">pilot_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">pilot_inputs</span><span class="p">))</span>
</pre></div>
</div>
<p>The covariance matrix is then computed using the <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code> <code class="docutils literal notranslate"><span class="pre">OutputProcessor</span></code> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">OutputProcessor</span><span class="o">.</span><span class="n">compute_covariance_matrix</span><span class="p">(</span><span class="n">pilot_outputs</span><span class="p">)</span>
</pre></div>
</div>
<p>At this point the necessary elements for computing optimal sample allocation are
now available: model costs, pilot outputs, and a covariance matrix.</p>
</div>
<div class="section" id="step-2-perform-sample-allocation-optimization">
<h2>Step 2: Perform sample allocation optimization<a class="headerlink" href="#step-2-perform-sample-allocation-optimization" title="Permalink to this headline">¶</a></h2>
<p>The sample allocation optimization with <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code> is now performed assuming a computational budget or target cost of 10000 seconds.</p>
<p>In the below snippet taken from the example code, all available optimization
algorithms are individually tested to find the method that produces the lowest
estimator variance. The Optimizer.optimize() method returns an instance of the <code class="docutils literal notranslate"><span class="pre">OptimizationResult</span></code> class with attributes for estimatore variance and optimal sample allocation. The sample allocation with the lowest variance will be used to generate an estimator in the subsequent steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target_cost</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">variance_results</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">sample_allocation_results</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="n">mxmc_optimizer</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span><span class="n">model_costs</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="p">)</span>

<span class="n">algorithms</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="o">.</span><span class="n">get_algorithm_names</span><span class="p">()</span>
<span class="k">for</span> <span class="n">algorithm</span> <span class="ow">in</span> <span class="n">algorithms</span><span class="p">:</span>

    <span class="n">opt_result</span> <span class="o">=</span> <span class="n">mxmc_optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">target_cost</span><span class="p">)</span>
    <span class="n">variance_results</span><span class="p">[</span><span class="n">algorithm</span><span class="p">]</span> <span class="o">=</span> <span class="n">opt_result</span><span class="o">.</span><span class="n">variance</span>
    <span class="n">sample_allocation_results</span><span class="p">[</span><span class="n">algorithm</span><span class="p">]</span> <span class="o">=</span> <span class="n">opt_result</span><span class="o">.</span><span class="n">allocation</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> method variance: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">opt_result</span><span class="o">.</span><span class="n">variance</span><span class="p">))</span>

<span class="n">best_method</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">variance_results</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">variance_results</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
<span class="n">sample_allocation</span> <span class="o">=</span> <span class="n">sample_allocation_results</span><span class="p">[</span><span class="n">best_method</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best method: &quot;</span><span class="p">,</span> <span class="n">best_method</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code> class also provides functionality for determining an optimal
subset of the models via the boolean parameter auto_model_selection of the
Optimizer.optimize() method. By default, all provided models are used. Note that enabling this option could take considerably longer as every
combination of the models will be tested.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mxmc_optimizer</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span><span class="n">model_costs</span><span class="p">,</span>
                           <span class="n">covariance_matrix</span><span class="p">,</span>
                           <span class="n">auto_model_selection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">opt_result</span> <span class="o">=</span> <span class="n">mxmc_optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">target_cost</span><span class="p">)</span>
<span class="n">variance_results</span> <span class="o">=</span> <span class="n">opt_result</span><span class="o">.</span><span class="n">variance</span>
<span class="n">sample_allocation_results</span> <span class="o">=</span> <span class="n">opt_result</span><span class="o">.</span><span class="n">allocation</span>
</pre></div>
</div>
</div>
<div class="section" id="step-3-generate-input-samples-for-models">
<h2>Step 3: Generate input samples for models<a class="headerlink" href="#step-3-generate-input-samples-for-models" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">SampleAllocation</span></code> class provides functionality for determining how many random input samples are needed and how they are to be allocated across the available models. This is demonstrated below for the optimal sample allocation object found in the previous step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_total_samples</span> <span class="o">=</span> <span class="n">sample_allocation</span><span class="o">.</span><span class="n">num_total_samples</span>
<span class="n">all_samples</span> <span class="o">=</span> <span class="n">get_uniform_sample_distribution</span><span class="p">(</span><span class="n">num_total_samples</span><span class="p">)</span> <span class="c1"># User code.</span>
<span class="n">model_input_samples</span> <span class="o">=</span> <span class="n">sample_allocation</span><span class="o">.</span><span class="n">allocate_samples_to_models</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span>
</pre></div>
</div>
<p>The num_total_samples property can be referenced for creation of an ndarray of
inputs, which can then be provided to the allocate_samples_to_models() method.
This method will redistribute the ndarray of input samples into a list of
ndarrays, each containing the prescribed number of samples for each model.</p>
</div>
<div class="section" id="step-4-compute-model-outputs-for-prescribed-inputs">
<h2>Step 4: Compute model outputs for prescribed inputs<a class="headerlink" href="#step-4-compute-model-outputs-for-prescribed-inputs" title="Permalink to this headline">¶</a></h2>
<p>Now that the input samples for each model have been generated and allocated,
outputs are generated by evaluating the Ishigami models as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model_input_samples</span><span class="p">,</span> <span class="n">models</span><span class="p">):</span>
    <span class="n">model_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">input_sample</span><span class="p">))</span>
</pre></div>
</div>
<p>The outputs are stored in a list of ndarrays corresponding to each model.</p>
<p>Note that for the practical case where evaluating the available models is time consuming and must be done in an <cite>offline</cite> fashion, the <code class="docutils literal notranslate"><span class="pre">SampleAllocation</span></code> class has functionality to save and load to/from disc. This way, the optimal sample allocation can be saved after Step 2 above and then loaded to generate an estimator in Step 5 next.</p>
</div>
<div class="section" id="step-5-form-estimator">
<h2>Step 5: Form estimator<a class="headerlink" href="#step-5-form-estimator" title="Permalink to this headline">¶</a></h2>
<p>Finally, an estimator for <span class="math notranslate nohighlight">\(E[f^{(1)}]\)</span> is computed using the <code class="docutils literal notranslate"><span class="pre">Estimator</span></code> class and the model outputs from the previous step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">Estimator</span><span class="p">(</span><span class="n">sample_allocation</span><span class="p">,</span> <span class="n">covariance_matrix</span><span class="p">)</span>
<span class="n">estimate</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">get_estimate</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Estimate = &quot;</span><span class="p">,</span> <span class="n">estimate</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the sample allocation object from Step 2 and the covariance matrix from Step 1 are required here. The covariance matrix could also be updated at this point using the model outputs generated in the previous step. The <code class="docutils literal notranslate"><span class="pre">MXMCPy</span></code> estimator is close to the true value of 2.5.</p>
<div class="line-block">
<div class="line">[1] “High-dimensional and higher-order multifidelity Monte Carlo estimators” (Quaglino, 2018)</div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">MXMC</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">End-to-End Example</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#step-1-compute-model-outputs-for-pilot-samples">Step 1: Compute model outputs for pilot samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-2-perform-sample-allocation-optimization">Step 2: Perform sample allocation optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-3-generate-input-samples-for-models">Step 3: Generate input samples for models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-4-compute-model-outputs-for-prescribed-inputs">Step 4: Compute model outputs for prescribed inputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-5-form-estimator">Step 5: Form estimator</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="source_code.html">Source Code Documentation</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="intro.html" title="previous chapter">Introduction</a></li>
      <li>Next: <a href="source_code.html" title="next chapter">Source Code Documentation</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, NASA.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.0.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/ishigami_example.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>