
Example - Ishigami
=============================

This example provides a simple demonstration of MXMCPy functionality. The
example code computes the estimate of three Ishigami models as described in
"High-dimensional and higher-order multifidelity Monte Carlo estimators"
(Quaglino, 2018). The goal is to estimate the expectation using the below models
with MXMCPy and compare to an analytically determined solution of E[F(z)],
which is computed to be 2.5.


.. math::
    \begin{gather*}
    f^{(1)}\ =\ sin(z_1)\ +\ 5\ sin^{2}(z_2)\ +\ \frac{1}{10}\ z_3^{4} sin(z_1) \\
    f^{(2)}\ =\ sin(z_1)\ +\ 4.75\ sin^{2}(z_2)\ +\ \frac{1}{10}\ z_3^{4} sin(z_1) \\
    f^{(3)}\ =\ sin(z_1)\ +\ 3\ sin^{2}(z_2)\ +\ \frac{9}{10}\ z_3^{2} sin(z_1) \\
    with\ \ z_i\sim\textit{U}(-\pi,\ \pi)
    \end{gather*} \\

The example covers all steps for utilizing MXMCPy, including computing model
outputs for pilot samples, performing sample allocation optimization,
generating input samples for each model, computing the outputs, and forming
the estimator.

The full source code for this example can be found in the MXMCPy repository:

``/examples/ishigami/run_ishigami.py``


Step 1: Compute model outputs for pilot samples
-----------------------------------------------

Begin by importing the necessary Python modules, including MXMCPy classes and Numpy:

.. code-block:: python

    import numpy as np

    from mxmc import Optimizer
    from mxmc import OutputProcessor
    from mxmc import Estimator


Below is the essential setup for the example. The user's model, IshigamiModel,
is imported. The number of pilot samples to take from each model is established,
along with the costs of each model. Three IshigamiModels are then instantiated
per the parameters in the equations mentioned previously and stored in a list
variable, models.

.. code-block:: python

    from ishigami_model import IshigamiModel

    num_pilot_samples = 10
    model_costs = np.array([1, .05, .001])

    high_fidelity_model =   IshigamiModel(a=5.,   b=.1, c=4.)
    medium_fidelity_model = IshigamiModel(a=4.75, b=.1, c=4.)
    low_fidelity_model =    IshigamiModel(a=3.,   b=.9, c=2.)
    models = [high_fidelity_model, medium_fidelity_model, low_fidelity_model]


The pilot samples are then computed using the Ishigami models with inputs
taken from uniform distributions on the range :math:`(-\pi, \pi)`. This step
does not require MXMCPy.

.. code-block:: python

    pilot_inputs = get_uniform_sample_distribution(num_pilot_samples)
    pilot_outputs = list()
    for model in models:
        pilot_outputs.append(model.evaluate(pilot_inputs))


MXMCPy provides a convenient function for computing a covariance matrix from the
sample outputs via the OutputProcessor class.

.. code-block:: python

    covariance_matrix = OutputProcessor.compute_covariance_matrix(pilot_outputs)


At this point the necessary elements for computing optimal sample allocation are
now available: model costs, pilot outputs, and a covariance matrix.

Step 2: Perform sample allocation optimization
--------------------------------------------------------------------------

Once the prerequisites for sample allocation optimization have been
established, the process can be performed by specifying a target cost and
either indicating a specific optimizer or testing each algorithm in turn.

In the below snippet taken from the provided example code, all available
algorithms are individually tested to find the method that produces the lowest
variance given the target cost. The Optimizer.optimize method returns an
instance of the SampleAllocation class containing information such as variance
and sample allocation. The instance of the SampleAllocation class with the
lowest variance is used for subsequent steps.

.. code-block:: python

    target_cost = 10000
    variance_results = dict()
    sample_allocation_results = dict()

    mxmc_optimizer = Optimizer(model_costs, covariance_matrix)

    algorithms = Optimizer.get_algorithm_names()
    for algorithm in algorithms:

        opt_result = mxmc_optimizer.optimize(algorithm, target_cost)
        variance_results[algorithm] = opt_result.variance
        sample_allocation_results[algorithm] = opt_result.allocation

        print("{} method variance: {}".format(algorithm, opt_result.variance))

    best_method = min(variance_results, key=variance_results.get)
    sample_allocation = sample_allocation_results[best_method]

    print("Best method: ", best_method)


The Optimizer class also provides functionality for determining an optimal
subset of the models via the boolean parameter auto_model_selection of the
Optimizer.optimize() method. By default, all provided models are used.

.. code-block:: python

    mxmc_optimizer = Optimizer(model_costs,
                               covariance_matrix,
                               auto_model_selection=True)

    opt_result = mxmc_optimizer.optimize(algorithm, target_cost)
    variance_results = opt_result.variance
    sample_allocation_results = opt_result.allocation


Step 3: Generate input samples for models
--------------------------------------------------------------

Once sample allocation and algorithm are determined, it is up to the user
to provide appropriate input samples for the models. The task is similar
to the creation of pilot samples, but uses the sample allocation data
from the previous step.

.. code-block:: python

    num_total_samples = sample_allocation.num_total_samples
    all_samples = get_uniform_sample_distribution(num_total_samples) # User code.
    model_input_samples = sample_allocation.allocate_samples_to_models(all_samples)


MXMCPy's SampleAllocation class provides tools that aid in this process.
The num_total_samples property can be referenced for creation of an ndarray of
inputs, which can then be provided to the allocate_samples_to_models method.
This method will redistribute the ndarray of input samples into a list of
ndarrays, each containing the prescribed number of samples for each model.

Step 4: Compute model outputs for prescribed inputs
---------------------------------------------------------------

Now that the number of samples for each model has been prescribed, the samples
must be generated. This should be a straightforward process:

.. code-block:: python

    model_outputs = list()
    for input_sample, model in zip(model_input_samples, models):
        model_outputs.append(model.evaluate(input_sample))


The outputs should be stored in a list of ndarrays corresponding to each model.

Step 5: Form estimator
--------------------------------------------------

Finally, the final estimator is computed using MXMCPy's Estimator class and the
computed model outputs. A covariance matrix computed from the model outputs
will be necessary, so the OutputProcessor's compute_covariance_matrix method
will be useful once again.

.. code-block:: python

    output_cov_matrix = OutputProcessor.compute_covariance_matrix(pilot_outputs)

    estimator = Estimator(sample_allocation, output_cov_matrix)
    estimate = estimator.get_estimate(model_outputs)

    print("Estimate = ", estimate)


The expectation for the model is 2.5, and is reliably approximated
by the example code.
